{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraer lista de identificadores de twitch de las categorias más vistas (Haciendo scrapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para la extracción de los identificadores de las categorias y los streamers\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src import soporte_extraccion as se\n",
    "\n",
    "# Para la extracción de datos ampliados\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "# Para la extracción de datos de las categorias asincrono\n",
    "from src import extraccion_async_info_categorias as eic\n",
    "\n",
    "# Para la extracción de datos de los streamers asincrono\n",
    "from src import extraccion_async_info_streamers as eis\n",
    "\n",
    "# Para el almacenamiento en base de datos\n",
    "from src import soporte_bbdd_mongo as sbm\n",
    "\n",
    "# Otros\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorias = se.extraer_lista_categorias(silent_mode=True)\n",
    "\n",
    "df_categorias = se.cambiar_tipos_columnas(df_categorias, df_categorias.columns.to_list())\n",
    "\n",
    "df_identificadores_categorias = se.obtener_identificadores(df_categorias)\n",
    "\n",
    "# Almacenamiento en base de datos\n",
    "client, db = sbm.conectar_mongo()\n",
    "sbm.insertar_en_coleccion(db, \"seven_viewers\", df_categorias)\n",
    "sbm.insertar_en_coleccion(db, \"id_categorias\", df_identificadores_categorias)\n",
    "client.close()\n",
    "\n",
    "# Almacenamiento en csv y pickle de los datos\n",
    "# df_identificadores_categorias.to_pickle(\"../datos/raw/categorias/identificadores/categories_id.pkl\")\n",
    "# df_identificadores_categorias.to_csv(\"../datos/raw/categorias/identificadores/categories_id.csv\")\n",
    "# df_categorias.to_pickle(\"../datos/raw/categorias/ultimos_7_dias/viewers_last_seven_days_all_categories.pkl\")\n",
    "# df_categorias.to_csv(\"../datos/raw/categorias/ultimos_7_dias/viewers_last_seven_days_all_categories.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ampliación de extracción de los datos\n",
    "# lista_elementos_clickar = [\"rating\", \"average-channels\", \"time-watched\", \"live\", \"peak-viewers\", \"peak-channels\", \"to-stream\"]\n",
    "lista_elementos_clickar = [\"rating\", \"average-channels\", \"time-watched\", \"peak-viewers\", \"peak-channels\", \"to-stream\"]\n",
    "client, db = sbm.conectar_mongo()\n",
    "for endpoint in lista_elementos_clickar:\n",
    "    print(f\"\\nIniciando {endpoint.capitalize()}\\n\")\n",
    "    inicio = time.time()\n",
    "    df_categorias = se.extraer_lista_categorias_ampliado(endpoint, silent_mode=True)\n",
    "    tiempo = round(time.time() - inicio, 1)\n",
    "    print(f\"\\nHa tardado {tiempo} segundos\\n\")\n",
    "    #display(df_categorias.head())\n",
    "    df_categorias.to_pickle(f\"../datos/raw/categorias/ultimos_7_dias/{endpoint.replace(\"-\", \"_\")}_last_seven_days_all_categories.pkl\")\n",
    "    df_categorias.to_csv(f\"../datos/raw/categorias/ultimos_7_dias/{endpoint.replace(\"-\", \"_\")}_last_seven_days_all_categories.csv\")\n",
    "    print(f\"\\nDatos almacenados en: datos/raw/categorias/ultimos_7_dias/{endpoint.replace(\"-\", \"_\")}_last_seven_days_all_categories.csv\\n\")\n",
    "    sbm.insertar_en_coleccion(db, f\"seven_{endpoint.replace(\"-\", \"_\")}\", df_categorias)\n",
    "    sleep(5)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de info sobre todas las categorias de la lista (Async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Este es el que funciona\n",
    "# import pandas as pd\n",
    "# import sys\n",
    "# sys.path.append(\"../\")\n",
    "# from src import extraccion_async_info_categorias as pa\n",
    "\n",
    "# %reload_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Margen de error con 8 nucleos y 16 retires = \n",
    "# # Margen de error con 4 nucleos y 16 retries = 901/930 (se quedó bloqueado en la última y no hizo el backup)\n",
    "# # Margen de error con 4 hilos y 4 retries = (Le he metido un timeout de 2 minutos)\n",
    "eic.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruta_backup = \"../datos/raw/categorias/backup/backup\"\n",
    "# ruta_carpeta = \"../datos/output/categorias/all\"\n",
    "# se.consolidar_backup(ruta_carpeta, ruta_backup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de info sobre las categorias de la lista (No async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_pickle = pd.read_pickle(f\"../datos/raw/categorias/identificadores/categories_id.pkl\")\n",
    "# df_identificadores_categorias = pd.DataFrame(data_pickle)\n",
    "# df_final = se.extraer_info_categoria_a_categoria(df_identificadores_categorias, silent_mode=True)\n",
    "# df_final.to_csv(\"../datos/raw/categorias/backup/prueba_backup_categorias.csv\")\n",
    "# df_final.to_pickle(\"../datos/raw/categorias/backup/prueba_backup_categorias.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de la lista de streamers (No async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [47:13<00:00, 56.66s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nombre</th>\n",
       "      <th>Enlace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Nombre, Enlace]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datos insertados o actualizados en la colección 'streamers' correctamente.\n"
     ]
    }
   ],
   "source": [
    "df_streamers = se.extraer_lista_streamers(True)\n",
    "display(df_streamers.head(-10))\n",
    "client, db = sbm.conectar_mongo(local=False)\n",
    "sbm.insertar_en_coleccion(db, \"streamers\", df_streamers)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streamers uno a uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pickle = pd.read_pickle(f\"../datos/raw/streamers/streamers_ranking.pkl\")\n",
    "df_streamers = pd.DataFrame(data_pickle)\n",
    "df_streamers = df_streamers[600:1400] # Hasta 1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [44:37<00:00, 38.25s/it]\n"
     ]
    }
   ],
   "source": [
    "df_datos_streamers = se.extraer_info_stremaer_a_streamer(df_streamers, silent_mode=True)\n",
    "# df_datos_streamers.to_pickle(f\"../datos/raw/streamers/backup/backup_datos_streamers.pkl\")\n",
    "# df_datos_streamers.to_csv(f\"../datos/raw/streamers/backup/backup_datos_streamers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamers async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eis.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruta_backup = \"../datos/raw/streamers/backup/backup\"\n",
    "# ruta_carpeta = \"../datos/output/streamers/all\"\n",
    "# se.consolidar_backup(ruta_carpeta, ruta_backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 44/2043 [00:17<15:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: El DataFrame no contiene la columna 'Nombre'. No se pueden asignar IDs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1882/2043 [12:22<01:09,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: El DataFrame no contiene la columna 'Nombre'. No se pueden asignar IDs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2043/2043 [13:25<00:00,  2.54it/s]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ruta de la carpeta que contiene los archivos CSV\n",
    "carpeta_csv = '../datos/output/streamers/all'  # <-- Cambia esta ruta según tu entorno\n",
    "\n",
    "# Genera la lista de todos los archivos CSV en la carpeta\n",
    "patron = os.path.join(carpeta_csv, '*.csv')\n",
    "lista_csv = glob.glob(patron)\n",
    "\n",
    "\n",
    "if not lista_csv:\n",
    "    print(\"No se encontraron archivos CSV en la carpeta especificada.\")\n",
    "else:\n",
    "    client, db = sbm.conectar_mongo(False)\n",
    "    # Procesa cada archivo CSV encontrado\n",
    "    for archivo in tqdm(lista_csv):\n",
    "        # nombre = os.path.splitext(os.path.basename(archivo))[0].strip().lower()\n",
    "        # nombre = nombre.split(\"_\")[2]\n",
    "        # df.insert(0, \"Nombre\", nombre)\n",
    "        df = pd.read_csv(archivo, index_col=0)\n",
    "\n",
    "        if \"id_streamer\" in df.columns:\n",
    "            df.drop(columns=[\"id_streamer\"], inplace=True)\n",
    "\n",
    "        if f\"% Gain\" in df.columns:\n",
    "            df.rename(columns={f\"% Gain\": \"PercentageGainHours\"}, inplace=True)\n",
    "            df[\"PercentageGainHours\"] = df[\"PercentageGainHours\"].str.replace(\"%\", \"\")\n",
    "\n",
    "        col_aux = df.copy()\n",
    "\n",
    "        if f\"FollowesGain\" in df.columns:\n",
    "            df.rename(columns={f\"FollowesGain\": \"FollowersGain\"}, inplace=True)\n",
    "            df[\"PercentageGainFollowers\"] = df[\"FollowersGain\"]\n",
    "            df[\"FollowersGain\"] = col_aux[\"PercentageGainFollowers\"]\n",
    "            df[\"PercentageGainFollowers\"] = df[\"PercentageGainFollowers\"].str.replace(\"%\", \"\").str.replace(\"+\", \"\").str.replace(r\"\\-\", \"0\", regex=True).str.replace(\",\", \"\").astype(float)\n",
    "            df[\"FollowersGain\"] = df[\"FollowersGain\"].astype(str).str.replace(\"+\", \"\").str.replace(\",\", \"\").str.replace(r\"\\-\", \"0\", regex=True).astype(int)\n",
    "            df[\"Followers\"] = df[\"Followers\"].astype(str).str.replace(\",\", \"\").astype(int)\n",
    "\n",
    "\n",
    "        df = se.asignar_ids_streamers(df)\n",
    "        \n",
    "        if df is not None:\n",
    "            sbm.insertar_en_coleccion(db, \"historico_streamers\", df, True, None)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    client.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2039/2039 [06:37<00:00,  5.13it/s]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ruta de la carpeta que contiene los archivos CSV\n",
    "carpeta_csv = '../datos/output/streamers/cats_streams/'  # <-- Cambia esta ruta según tu entorno\n",
    "\n",
    "# Genera la lista de todos los archivos CSV en la carpeta\n",
    "patron = os.path.join(carpeta_csv, '*.csv')\n",
    "lista_csv = glob.glob(patron)\n",
    "\n",
    "\n",
    "if not lista_csv:\n",
    "    print(\"No se encontraron archivos CSV en la carpeta especificada.\")\n",
    "else:\n",
    "    client, db = sbm.conectar_mongo(False)\n",
    "    # Procesa cada archivo CSV encontrado\n",
    "    for archivo in tqdm(lista_csv):\n",
    "        # nombre = os.path.splitext(os.path.basename(archivo))[0].strip().lower()\n",
    "        # nombre = nombre.split(\"_\")[2]\n",
    "        # df.insert(0, \"Nombre\", nombre)\n",
    "        df = pd.read_csv(archivo, index_col=0)\n",
    "\n",
    "        if \"id_streamer\" in df.columns:\n",
    "            df.drop(columns=[\"id_streamer\"], inplace=True)\n",
    "\n",
    "        if f\"% Gain\" in df.columns:\n",
    "            df.rename(columns={f\"% Gain\": \"PercentageGainHours\"}, inplace=True)\n",
    "\n",
    "        if f\"FollowesGain\" in df.columns:\n",
    "            df.rename(columns={f\"FollowesGain\": \"FollowersGain\"}, inplace=True)\n",
    "\n",
    "        df = se.asignar_ids_streamers(df)\n",
    "\n",
    "        # # Solo subo la primera categoria\n",
    "        # df_categorias_group = df.groupby(\"id_streamer\").first().reset_index()\n",
    "        \n",
    "        if df is not None:\n",
    "            sbm.insertar_en_coleccion(db, \"categorias_streameadas\", df, True, None)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ruta de la carpeta que contiene los archivos CSV\n",
    "archivo = '../datos/raw/streamers/streamers_ranking.pkl'  # <-- Cambia esta ruta según tu entorno\n",
    "\n",
    "client, db = sbm.conectar_mongo()\n",
    "# nombre = os.path.splitext(os.path.basename(archivo))[0].strip().lower()\n",
    "# nombre = nombre.split(\"_\")[2]\n",
    "# df.insert(0, \"Nombre\", nombre)\n",
    "data_pickle = pd.read_pickle(archivo)\n",
    "df = pd.DataFrame(data_pickle)\n",
    "\n",
    "if \"id_streamer\" in df.columns:\n",
    "    df.drop(columns=[\"id_streamer\"], inplace=True)\n",
    "\n",
    "if f\"% Gain\" in df.columns:\n",
    "    df.rename(columns={f\"% Gain\": \"PercentageGainHours\"}, inplace=True)\n",
    "\n",
    "if f\"FollowesGain\" in df.columns:\n",
    "    df.rename(columns={f\"FollowesGain\": \"FollowersGain\"}, inplace=True)\n",
    "\n",
    "df = se.asignar_ids_streamers(df)\n",
    "\n",
    "sbm.insertar_en_coleccion(db, \"streamers\", df, True, \"id_streamer\")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracción de datos adicionales desde la API\n",
    "import requests\n",
    "\n",
    "url = \"https://api.twitchtracker.com/v1/endpoint\"\n",
    "response = requests.get(url)\n",
    "datos = response.json()\n",
    "client, db = sbm.conectar_mongo(local=False)\n",
    "sbm.insertar_en_coleccion(db, \"last_30_days\", datos, clave_unica=\"nombre\")\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProtocolError",
     "evalue": "('Connection aborted.', ConnectionResetError(10054, 'Se ha forzado la interrupción de una conexión existente por el host remoto', None, 10054, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n    response = conn.getresponse()\n               ^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n    httplib_response = super().getresponse()\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\http\\client.py\", line 1428, in getresponse\n    response.begin()\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\http\\client.py\", line 331, in begin\n    version, status, reason = self._read_status()\n                              ^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\http\\client.py\", line 292, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\socket.py\", line 720, in readinto\n    return self._sock.recv_into(b)\n           ^^^^^^^^^^^^^^^^^^^^^^^\nConnectionResetError: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ProyectoFinalData\\proyecto_final\\notebooks\\..\\src\\extraccion_categorias_horas.py\", line 49, in extraer_datos_streamer\n    driver.find_element(\"xpath\", \"/html/body/div[4]/div[2]/div[2]/div[3]/div[2]/button[2]/p\").click()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\seleniumbase\\core\\sb_driver.py\", line 27, in find_element\n    return self.driver.default_find_element(by=by, value=value)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\", line 770, in find_element\n    return self.execute(Command.FIND_ELEMENT, {\"using\": by, \"value\": value})[\"value\"]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\", line 382, in execute\n    response = self.command_executor.execute(driver_command, params)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\", line 404, in execute\n    return self._request(command_info[0], url, body=data)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\", line 428, in _request\n    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\urllib3\\_request_methods.py\", line 143, in request\n    return self.request_encode_body(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\urllib3\\_request_methods.py\", line 278, in request_encode_body\n    return self.urlopen(method, url, **extra_kw)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\urllib3\\poolmanager.py\", line 443, in urlopen\n    response = conn.urlopen(method, u.request_uri, **kw)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n    retries = retries.increment(\n              ^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 474, in increment\n    raise reraise(type(error), error, _stacktrace)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\urllib3\\util\\util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n    response = conn.getresponse()\n               ^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n    httplib_response = super().getresponse()\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\http\\client.py\", line 1428, in getresponse\n    response.begin()\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\http\\client.py\", line 331, in begin\n    version, status, reason = self._read_status()\n                              ^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\http\\client.py\", line 292, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\socket.py\", line 720, in readinto\n    return self._sock.recv_into(b)\n           ^^^^^^^^^^^^^^^^^^^^^^^\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(10054, 'Se ha forzado la interrupción de una conexión existente por el host remoto', None, 10054, None))\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m df_streamers \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../datos/raw/streamers/ids_streamers.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m df_datos, df_categorias \u001b[38;5;241m=\u001b[39m \u001b[43mech\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextraer_info_streamer_joblib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_streamers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcatenar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ProyectoFinalData\\proyecto_final\\notebooks\\..\\src\\extraccion_categorias_horas.py:158\u001b[0m, in \u001b[0;36mextraer_info_streamer_joblib\u001b[1;34m(df_streamers, concatenar, silent_mode)\u001b[0m\n\u001b[0;32m    155\u001b[0m num_procesos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m  \u001b[38;5;66;03m# Ajusta según tu hardware\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# Extraer datos principales de los streamers en paralelo\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m resultados_datos \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_procesos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextraer_datos_streamer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent_mode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf_streamers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNombre\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# Extraer categorías de los streamers en paralelo\u001b[39;00m\n\u001b[0;32m    163\u001b[0m resultados_categorias \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mnum_procesos)(\n\u001b[0;32m    164\u001b[0m     delayed(extraer_categorias_streamer)(streamer, silent_mode) \u001b[38;5;28;01mfor\u001b[39;00m streamer \u001b[38;5;129;01min\u001b[39;00m df_streamers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNombre\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    165\u001b[0m )\n",
      "File \u001b[1;32md:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32md:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\ArchivosPrograma\\anaconda3\\envs\\proyecto_final\\Lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, 'Se ha forzado la interrupción de una conexión existente por el host remoto', None, 10054, None))"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src import extraccion_categorias_horas as ech\n",
    "import pandas as pd\n",
    "\n",
    "df_streamers = pd.read_csv(\"../datos/raw/streamers/ids_streamers.csv\").head(5)\n",
    "df_datos, df_categorias = ech.extraer_info_streamer_joblib(df_streamers, concatenar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
